{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqBxJO8hcUco",
        "outputId": "9c8a926c-38d3-40a1-f59b-b59f8b9b9c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import entropy\n",
        "\n",
        "\n",
        "# Définir les scénarios et le chemin d'accès\n",
        "training_scenarios = [9]\n",
        "base_path = \"/content/drive/MyDrive/Training scenarios/\"\n",
        "\n",
        "training_files = []\n",
        "\n",
        "for scenario_id in training_scenarios:\n",
        "    file_name = f\"{base_path}scenario{scenario_id}.binetflow\"\n",
        "    training_files.append(pd.read_csv(file_name, delimiter=\",\", low_memory=False))\n",
        "\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# 1. Prétraitement des données\n",
        "# ==========================\n",
        "def preprocess_flags(df, is_test=False):\n",
        "    \"\"\"\n",
        "    Convertit le champ 'StartTime' en datetime, filtre les flux\n",
        "    avant les 25 premières minutes, convertit les labels en 0/1 et\n",
        "    conserve uniquement les flux TCP. Renomme 'State' en 'Flags' si nécessaire.\n",
        "    \"\"\"\n",
        "    df['StartTime'] = pd.to_datetime(df['StartTime'])\n",
        "    min_time = df['StartTime'].min()\n",
        "    threshold_time = min_time + pd.Timedelta(minutes=25)\n",
        "    df = df[df['StartTime'] >= threshold_time].copy()\n",
        "    print(\"Après filtrage temporel, forme du DataFrame :\", df.shape)\n",
        "\n",
        "    # Convertir le label : 1 si \"botnet\" est présent dans le label, sinon 0\n",
        "    df['Label'] = df['Label'].apply(lambda x: 1 if 'botnet' in str(x).lower() else 0)\n",
        "\n",
        "    # Normaliser et filtrer le protocole pour ne garder que TCP\n",
        "    df['Proto'] = df['Proto'].str.strip().str.lower()\n",
        "    df = df[df['Proto'] == 'tcp'].copy()\n",
        "\n",
        "    # Renommer la colonne 'State' en 'Flags' si nécessaire\n",
        "    if 'State' in df.columns and 'Flags' not in df.columns:\n",
        "        df.rename(columns={'State': 'Flags'}, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def get_rare_combos(df, rare_threshold=0.01):\n",
        "    \"\"\"\n",
        "    Retourne une liste des combinaisons de flags rares (qui apparaissent en dessous d'un seuil).\n",
        "    \"\"\"\n",
        "    combo_counts = df['Flags'].value_counts(normalize=True)\n",
        "    rare_combos = combo_counts[combo_counts < rare_threshold].index.tolist()\n",
        "    return rare_combos\n",
        "\n",
        "# ==========================\n",
        "# 2. Extraction de caractéristiques améliorée (avec entropie temporelle)\n",
        "# ==========================\n",
        "def compute_temporal_flag_entropy(df, window_size=10, step=1):\n",
        "    \"\"\"\n",
        "    Calcule l'entropie moyenne temporelle des combinaisons de flags TCP pour chaque adresse IP.\n",
        "\n",
        "    Paramètres :\n",
        "        df (DataFrame) : Données prétraitées contenant les colonnes 'SrcAddr', 'StartTime', et 'Flags'.\n",
        "        window_size (int) : Taille de la fenêtre glissante utilisée pour calculer l'entropie.\n",
        "        step (int) : Décalage entre chaque fenêtre glissante.\n",
        "\n",
        "    Retourne :\n",
        "        Series : Entropie moyenne pour chaque IP, indiquant la variabilité temporelle des flags.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialiser un dictionnaire pour stocker les entropies moyennes par IP\n",
        "    ip_entropy = {}\n",
        "\n",
        "    # Regrouper les flux par adresse IP source\n",
        "    for ip, group in df.groupby('SrcAddr'):\n",
        "\n",
        "        # Trier les flux par ordre chronologique pour capturer l'évolution temporelle\n",
        "        group_sorted = group.sort_values('StartTime')\n",
        "\n",
        "        # Initialiser la liste qui stockera les entropies pour chaque fenêtre glissante\n",
        "        entropies = []\n",
        "\n",
        "        # Extraire la séquence chronologique des flags TCP\n",
        "        flags_list = group_sorted['Flags'].tolist()\n",
        "\n",
        "        # Parcourir la liste des flags avec une fenêtre glissante\n",
        "        for i in range(0, len(flags_list) - window_size + 1, step):\n",
        "\n",
        "            # Sélectionner une fenêtre de flags consécutifs\n",
        "            window = flags_list[i:i + window_size]\n",
        "\n",
        "            # Calculer la fréquence de chaque combinaison de flags dans la fenêtre\n",
        "            freq = {}\n",
        "            for f in window:\n",
        "                freq[f] = freq.get(f, 0) + 1\n",
        "\n",
        "            # Normaliser les fréquences pour obtenir une distribution de probabilités\n",
        "            total = sum(freq.values())\n",
        "            norm_freq = [count / total for count in freq.values()]\n",
        "\n",
        "            # Calculer l'entropie (base 2) de cette distribution de probabilités\n",
        "            ent = entropy(norm_freq, base=2)\n",
        "\n",
        "            # Ajouter l'entropie calculée à la liste des entropies de cette IP\n",
        "            entropies.append(ent)\n",
        "\n",
        "        # Stocker l'entropie moyenne calculée pour cette IP\n",
        "        ip_entropy[ip] = np.mean(entropies) if entropies else 0\n",
        "\n",
        "    # Retourner les résultats sous forme de série Pandas\n",
        "    return pd.Series(ip_entropy, name='temporal_flag_entropy')\n",
        "\n",
        "\n",
        "def extract_features_enhanced(df, rare_combos):\n",
        "    \"\"\"\n",
        "    Extrait un ensemble de caractéristiques enrichies pour chaque IP.\n",
        "      - Indicateurs binaires pour les flags (A, S, F, R, P)\n",
        "      - Ratio SYN/FIN\n",
        "      - Nombre de flux par IP\n",
        "      - Durée moyenne des flux\n",
        "      - Entropie globale des combinaisons de flags\n",
        "      - Entropie temporelle (basée sur des fenêtres glissantes)\n",
        "      - Histogramme normalisé des combinaisons de flags\n",
        "    \"\"\"\n",
        "    # Création des indicateurs binaires pour chaque flag\n",
        "    flags = ['A', 'S', 'F', 'R', 'P']\n",
        "    for flag in flags:\n",
        "        df[f'has_{flag}'] = df['Flags'].apply(lambda x: 1 if flag in x else 0)\n",
        "\n",
        "    # Agrégation par IP : moyenne des indicateurs binaires\n",
        "    individual_features = df.groupby('SrcAddr')[['has_A', 'has_S', 'has_F', 'has_R', 'has_P']].mean()\n",
        "\n",
        "    # Calcul du ratio SYN/FIN\n",
        "    syn_count = df.groupby('SrcAddr')['has_S'].sum()\n",
        "    fin_count = df.groupby('SrcAddr')['has_F'].sum()\n",
        "    syn_fin_ratio = (syn_count / (fin_count + 1e-9)).rename('syn_fin_ratio')\n",
        "\n",
        "    # Nombre de flux par IP\n",
        "    flow_count = df.groupby('SrcAddr').size().rename('flow_count')\n",
        "\n",
        "    # Durée moyenne des flux par IP\n",
        "    avg_duration = df.groupby('SrcAddr')['Dur'].mean().rename('avg_duration')\n",
        "\n",
        "    # Construction de l'histogramme des combinaisons de flags\n",
        "    df['FlagCombo'] = df['Flags'].apply(lambda x: 'Other' if x in rare_combos else x)\n",
        "    combo_hist = df.groupby(['SrcAddr', 'FlagCombo']).size().unstack(fill_value=0)\n",
        "    total_flows = combo_hist.sum(axis=1)\n",
        "    norm_combo_hist = combo_hist.div(total_flows, axis=0)\n",
        "\n",
        "    # Entropie globale basée sur l'histogramme normalisé\n",
        "    global_entropy = norm_combo_hist.apply(lambda row: entropy(row, base=2), axis=1).rename('flag_entropy')\n",
        "\n",
        "    # Calcul de l'entropie temporelle à l'aide de fenêtres glissantes\n",
        "    temporal_entropy = compute_temporal_flag_entropy(df, window_size=10, step=1)\n",
        "\n",
        "    # Combinaison de toutes les caractéristiques\n",
        "    features = pd.concat([individual_features, syn_fin_ratio, flow_count, avg_duration, global_entropy, temporal_entropy], axis=1)\n",
        "    features = pd.concat([features, norm_combo_hist], axis=1).fillna(0)\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "3810QgMQ-vbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# 3. Entraînement du modèle hybride (PCA + Classification)\n",
        "# ==========================\n",
        "def train_hybrid_model(train_dfs, n_components=10):\n",
        "    \"\"\"\n",
        "    Prépare les données, extrait les caractéristiques enrichies, puis :\n",
        "      - Agrège les labels par IP (1 si au moins un flux est malveillant)\n",
        "      - Standardise les caractéristiques\n",
        "      - Applique le PCA pour réduire la dimensionnalité\n",
        "      - Entraîne une régression logistique sur les données projetées\n",
        "    Retourne le PCA, la liste des combinaisons rares, le scaler, le classifieur,\n",
        "    les caractéristiques standardisées et les labels agrégés par IP.\n",
        "    \"\"\"\n",
        "    full_train = pd.concat([preprocess_flags(df) for df in train_dfs])\n",
        "    rare_combos = get_rare_combos(full_train)\n",
        "    features = extract_features_enhanced(full_train, rare_combos)\n",
        "\n",
        "    # Agréger les labels par IP (au moins un flux malveillant => label 1)\n",
        "    ip_labels = full_train.groupby('SrcAddr')['Label'].max()\n",
        "    features = features.loc[ip_labels.index]\n",
        "\n",
        "    # Standardisation des caractéristiques\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "    features_scaled = pd.DataFrame(features_scaled, index=features.index, columns=features.columns)\n",
        "\n",
        "    # Application du PCA sur les caractéristiques standardisées\n",
        "    pca = PCA(n_components=n_components, svd_solver='full')\n",
        "    pca.fit(features_scaled)\n",
        "    features_pca = pca.transform(features_scaled)\n",
        "\n",
        "    # Entraînement du classifieur (régression logistique) sur l'espace PCA\n",
        "    clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "    clf.fit(features_pca, ip_labels.loc[features.index])\n",
        "\n",
        "    # Calcul de l'erreur de reconstruction pour référence\n",
        "    features_reconstructed = pca.inverse_transform(features_pca)\n",
        "    reconstruction_errors = np.linalg.norm(features_scaled - features_reconstructed, axis=1)\n",
        "    print(\"Erreur de reconstruction moyenne sur l'entraînement :\", np.mean(reconstruction_errors))\n",
        "\n",
        "    return pca, rare_combos, scaler, clf, features_scaled, ip_labels\n"
      ],
      "metadata": {
        "id": "JPUXoqD5An1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# 4. Détection via la méthode hybride\n",
        "# ==========================\n",
        "def detect_anomalies_hybrid(test_df, pca, rare_combos, scaler, clf, train_feature_columns, norm_type='l2'):\n",
        "    \"\"\"\n",
        "    Traite les données de test : prétraitement, extraction et standardisation des caractéristiques,\n",
        "    projection dans l'espace PCA et utilisation du classifieur pour prédire la probabilité d'être malveillant.\n",
        "    Retourne les probabilités d'anomalie, le DataFrame prétraité et l'erreur de reconstruction.\n",
        "    \"\"\"\n",
        "    preprocessed_test = preprocess_flags(test_df, is_test=True)\n",
        "    test_features = extract_features_enhanced(preprocessed_test, rare_combos)\n",
        "    test_features = test_features.reindex(columns=train_feature_columns, fill_value=0)\n",
        "\n",
        "    # Appliquer le scaler sur les données de test\n",
        "    test_features_scaled = scaler.transform(test_features)\n",
        "    test_features_scaled = pd.DataFrame(test_features_scaled, index=test_features.index, columns=test_features.columns)\n",
        "\n",
        "    # Projection dans l'espace PCA\n",
        "    test_features_pca = pca.transform(test_features_scaled)\n",
        "\n",
        "    # Prédiction de la probabilité d'être malveillant via la régression logistique\n",
        "    anomaly_prob = clf.predict_proba(test_features_pca)[:, 1]\n",
        "    anomaly_prob = pd.Series(anomaly_prob, index=test_features.index)\n",
        "\n",
        "    # Calcul optionnel de l'erreur de reconstruction (norme L2 ou L1)\n",
        "    if norm_type == 'l2':\n",
        "        recon_error = np.linalg.norm(test_features_scaled - pca.inverse_transform(test_features_pca), axis=1)\n",
        "    elif norm_type == 'l1':\n",
        "        recon_error = np.sum(np.abs(test_features_scaled - pca.inverse_transform(test_features_pca)), axis=1)\n",
        "    else:\n",
        "        raise ValueError(\"norm_type must be either 'l2' or 'l1'\")\n",
        "    recon_error = pd.Series(recon_error, index=test_features.index)\n",
        "\n",
        "    return anomaly_prob, preprocessed_test, recon_error\n"
      ],
      "metadata": {
        "id": "12SC51ePIH3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# 5. Filtrage supplémentaire par nombre minimum de flux par IP\n",
        "# ==========================\n",
        "def filter_by_flow_count(preprocessed_test, min_flows=50):\n",
        "    \"\"\"\n",
        "    Retourne la liste des IP ayant au moins 'min_flows' flux.\n",
        "    \"\"\"\n",
        "    flow_counts = preprocessed_test.groupby('SrcAddr').size()\n",
        "    valid_ips = flow_counts[flow_counts >= min_flows].index\n",
        "    return valid_ips"
      ],
      "metadata": {
        "id": "KkB0fBUmJtbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# 6. Fonction d'évaluation\n",
        "# ==========================\n",
        "def evaluate_performance(scores, preprocessed_test, threshold):\n",
        "    \"\"\"\n",
        "    Évalue la performance en agrégant les labels par IP (vrai label = max(Label)) et en comparant\n",
        "    aux prédictions basées sur le seuil.\n",
        "    Retourne les métriques et la matrice de confusion.\n",
        "    \"\"\"\n",
        "    y_true = preprocessed_test.groupby('SrcAddr')['Label'].max()\n",
        "    y_pred = (scores > threshold).astype(int)\n",
        "    y_true, y_pred = y_true.align(y_pred, join='right', fill_value=0)\n",
        "\n",
        "    TP = np.sum((y_pred == 1) & (y_true == 1))\n",
        "    FP = np.sum((y_pred == 1) & (y_true == 0))\n",
        "    TN = np.sum((y_pred == 0) & (y_true == 0))\n",
        "    FN = np.sum((y_pred == 0) & (y_true == 1))\n",
        "\n",
        "    epsilon = 1e-9\n",
        "    metrics = {\n",
        "        'FPR': FP / (TN + FP + epsilon),\n",
        "        'TPR': TP / (TP + FN + epsilon),\n",
        "        'TNR': TN / (TN + FP + epsilon),\n",
        "        'FNR': FN / (TP + FN + epsilon),\n",
        "        'Precision': TP / (TP + FP + epsilon),\n",
        "        'Accuracy': (TP + TN) / (TP + TN + FP + FN + epsilon),\n",
        "        'ErrorRate': (FP + FN) / (TP + TN + FP + FN + epsilon),\n",
        "        'F1-Score': 2 * TP / (2 * TP + FP + FN + epsilon)\n",
        "    }\n",
        "    return metrics, (TP, FP, TN, FN)"
      ],
      "metadata": {
        "id": "JVhNBLeDXQeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# 7. Fonction d'optimisation du seuil\n",
        "# ==========================\n",
        "def optimize_threshold(scores, preprocessed_test, percentiles=np.arange(10, 100, 1)):\n",
        "    \"\"\"\n",
        "    Parcourt une plage de percentiles pour fixer un seuil qui maximise le F1-score.\n",
        "    Retourne le meilleur seuil, les métriques associées et la matrice de confusion.\n",
        "    \"\"\"\n",
        "    best_threshold = None\n",
        "    best_f1 = -1\n",
        "    best_metrics = None\n",
        "    best_conf = None\n",
        "\n",
        "    for perc in percentiles:\n",
        "        threshold = np.percentile(scores, perc)\n",
        "        metrics, conf = evaluate_performance(scores, preprocessed_test, threshold)\n",
        "        if metrics['F1-Score'] > best_f1:\n",
        "            best_f1 = metrics['F1-Score']\n",
        "            best_threshold = threshold\n",
        "            best_metrics = metrics\n",
        "            best_conf = conf\n",
        "        print(f\"Percentile: {perc}, Seuil: {threshold:.4f}, F1-Score: {metrics['F1-Score']:.4f}\")\n",
        "\n",
        "    print(\"\\nSeuil optimal:\", best_threshold)\n",
        "    return best_threshold, best_metrics, best_conf"
      ],
      "metadata": {
        "id": "xobHweN9IiJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test file\n",
        "scenario_test_id = 9\n",
        "test_df = pd.read_csv(f\"{base_path}scenario{scenario_test_id}.binetflow\", delimiter=\",\", low_memory=False)\n",
        "\n",
        "# ---- Phase d'entraînement ----\n",
        "pca, rare_combos, scaler, clf, train_features_scaled, ip_labels = train_hybrid_model(training_files, n_components=10)\n",
        "print(\"Modèle hybride entraîné. Forme des features d'entraînement :\", train_features_scaled.shape)\n",
        "\n",
        "# ---- Phase de détection ----\n",
        "anomaly_prob, preprocessed_test, recon_error = detect_anomalies_hybrid(test_df, pca, rare_combos, scaler, clf, train_features_scaled.columns, norm_type='l2')\n",
        "print(\"Probabilités d'anomalie (aperçu) :\\n\", anomaly_prob.head())\n",
        "\n",
        "# ---- Filtrage supplémentaire par nombre de flux ----\n",
        "valid_ips = filter_by_flow_count(preprocessed_test, min_flows=50)\n",
        "filtered_scores = anomaly_prob[anomaly_prob.index.isin(valid_ips)]\n",
        "filtered_preprocessed_test = preprocessed_test[preprocessed_test['SrcAddr'].isin(valid_ips)]\n",
        "\n",
        "# ---- Optimisation du seuil ----\n",
        "best_threshold, best_metrics, best_conf_matrix = optimize_threshold(filtered_scores, filtered_preprocessed_test, percentiles=np.arange(10, 100, 1))\n",
        "\n",
        "print(\"\\n=== Matrice de confusion optimale ===\")\n",
        "print(f\"TP: {best_conf_matrix[0]}, FP: {best_conf_matrix[1]}\")\n",
        "print(f\"FN: {best_conf_matrix[2]}, TN: {best_conf_matrix[3]}\\n\")\n",
        "print(\"=== Métriques de performance optimales ===\")\n",
        "metrics_df = pd.DataFrame([best_metrics], index=['Hybrid FLAGS PCA'])\n",
        "print(metrics_df.round(4).T.to_markdown())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdrd6IgYXRW6",
        "outputId": "4bb0b6d4-c380-4a4e-da84-4af41c9e85b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Après filtrage temporel, forme du DataFrame : (2087505, 15)\n",
            "Erreur de reconstruction moyenne sur l'entraînement : 1.3186394607139016\n",
            "Modèle hybride entraîné. Forme des features d'entraînement : (16938, 20)\n",
            "Après filtrage temporel, forme du DataFrame : (2087505, 15)\n",
            "Probabilités d'anomalie (aperçu) :\n",
            " 1.144.169.55     1.896796e-10\n",
            "1.144.233.103    3.625273e-11\n",
            "1.144.62.178     3.980249e-13\n",
            "1.148.65.189     3.486458e-12\n",
            "1.152.83.190     7.412210e-13\n",
            "dtype: float64\n",
            "Percentile: 10, Seuil: 0.0000, F1-Score: 0.0775\n",
            "Percentile: 11, Seuil: 0.0000, F1-Score: 0.0784\n",
            "Percentile: 12, Seuil: 0.0000, F1-Score: 0.0794\n",
            "Percentile: 13, Seuil: 0.0000, F1-Score: 0.0800\n",
            "Percentile: 14, Seuil: 0.0000, F1-Score: 0.0810\n",
            "Percentile: 15, Seuil: 0.0000, F1-Score: 0.0820\n",
            "Percentile: 16, Seuil: 0.0000, F1-Score: 0.0830\n",
            "Percentile: 17, Seuil: 0.0000, F1-Score: 0.0837\n",
            "Percentile: 18, Seuil: 0.0000, F1-Score: 0.0847\n",
            "Percentile: 19, Seuil: 0.0000, F1-Score: 0.0858\n",
            "Percentile: 20, Seuil: 0.0000, F1-Score: 0.0870\n",
            "Percentile: 21, Seuil: 0.0000, F1-Score: 0.0877\n",
            "Percentile: 22, Seuil: 0.0000, F1-Score: 0.0889\n",
            "Percentile: 23, Seuil: 0.0000, F1-Score: 0.0901\n",
            "Percentile: 24, Seuil: 0.0000, F1-Score: 0.0913\n",
            "Percentile: 25, Seuil: 0.0000, F1-Score: 0.0922\n",
            "Percentile: 26, Seuil: 0.0000, F1-Score: 0.0935\n",
            "Percentile: 27, Seuil: 0.0000, F1-Score: 0.0948\n",
            "Percentile: 28, Seuil: 0.0000, F1-Score: 0.0962\n",
            "Percentile: 29, Seuil: 0.0000, F1-Score: 0.0971\n",
            "Percentile: 30, Seuil: 0.0000, F1-Score: 0.0985\n",
            "Percentile: 31, Seuil: 0.0000, F1-Score: 0.1000\n",
            "Percentile: 32, Seuil: 0.0000, F1-Score: 0.1015\n",
            "Percentile: 33, Seuil: 0.0000, F1-Score: 0.1026\n",
            "Percentile: 34, Seuil: 0.0000, F1-Score: 0.1042\n",
            "Percentile: 35, Seuil: 0.0000, F1-Score: 0.1058\n",
            "Percentile: 36, Seuil: 0.0000, F1-Score: 0.1075\n",
            "Percentile: 37, Seuil: 0.0000, F1-Score: 0.1087\n",
            "Percentile: 38, Seuil: 0.0000, F1-Score: 0.1105\n",
            "Percentile: 39, Seuil: 0.0000, F1-Score: 0.1124\n",
            "Percentile: 40, Seuil: 0.0000, F1-Score: 0.1143\n",
            "Percentile: 41, Seuil: 0.0000, F1-Score: 0.1156\n",
            "Percentile: 42, Seuil: 0.0000, F1-Score: 0.1176\n",
            "Percentile: 43, Seuil: 0.0000, F1-Score: 0.1198\n",
            "Percentile: 44, Seuil: 0.0000, F1-Score: 0.1220\n",
            "Percentile: 45, Seuil: 0.0000, F1-Score: 0.1235\n",
            "Percentile: 46, Seuil: 0.0000, F1-Score: 0.1258\n",
            "Percentile: 47, Seuil: 0.0000, F1-Score: 0.1282\n",
            "Percentile: 48, Seuil: 0.0000, F1-Score: 0.1307\n",
            "Percentile: 49, Seuil: 0.0000, F1-Score: 0.1325\n",
            "Percentile: 50, Seuil: 0.0000, F1-Score: 0.1351\n",
            "Percentile: 51, Seuil: 0.0000, F1-Score: 0.1379\n",
            "Percentile: 52, Seuil: 0.0000, F1-Score: 0.1408\n",
            "Percentile: 53, Seuil: 0.0000, F1-Score: 0.1429\n",
            "Percentile: 54, Seuil: 0.0000, F1-Score: 0.1460\n",
            "Percentile: 55, Seuil: 0.0000, F1-Score: 0.1493\n",
            "Percentile: 56, Seuil: 0.0000, F1-Score: 0.1527\n",
            "Percentile: 57, Seuil: 0.0000, F1-Score: 0.1550\n",
            "Percentile: 58, Seuil: 0.0000, F1-Score: 0.1587\n",
            "Percentile: 59, Seuil: 0.0000, F1-Score: 0.1626\n",
            "Percentile: 60, Seuil: 0.0000, F1-Score: 0.1667\n",
            "Percentile: 61, Seuil: 0.0000, F1-Score: 0.1695\n",
            "Percentile: 62, Seuil: 0.0000, F1-Score: 0.1739\n",
            "Percentile: 63, Seuil: 0.0000, F1-Score: 0.1786\n",
            "Percentile: 64, Seuil: 0.0000, F1-Score: 0.1835\n",
            "Percentile: 65, Seuil: 0.0000, F1-Score: 0.1869\n",
            "Percentile: 66, Seuil: 0.0000, F1-Score: 0.1923\n",
            "Percentile: 67, Seuil: 0.0000, F1-Score: 0.1980\n",
            "Percentile: 68, Seuil: 0.0000, F1-Score: 0.2041\n",
            "Percentile: 69, Seuil: 0.0000, F1-Score: 0.2083\n",
            "Percentile: 70, Seuil: 0.0000, F1-Score: 0.2151\n",
            "Percentile: 71, Seuil: 0.0000, F1-Score: 0.2222\n",
            "Percentile: 72, Seuil: 0.0000, F1-Score: 0.2299\n",
            "Percentile: 73, Seuil: 0.0000, F1-Score: 0.2353\n",
            "Percentile: 74, Seuil: 0.0000, F1-Score: 0.2439\n",
            "Percentile: 75, Seuil: 0.0000, F1-Score: 0.2532\n",
            "Percentile: 76, Seuil: 0.0000, F1-Score: 0.2632\n",
            "Percentile: 77, Seuil: 0.0000, F1-Score: 0.2703\n",
            "Percentile: 78, Seuil: 0.0000, F1-Score: 0.2817\n",
            "Percentile: 79, Seuil: 0.0000, F1-Score: 0.2941\n",
            "Percentile: 80, Seuil: 0.0000, F1-Score: 0.3077\n",
            "Percentile: 81, Seuil: 0.0000, F1-Score: 0.3175\n",
            "Percentile: 82, Seuil: 0.0000, F1-Score: 0.3333\n",
            "Percentile: 83, Seuil: 0.0001, F1-Score: 0.3509\n",
            "Percentile: 84, Seuil: 0.0001, F1-Score: 0.3704\n",
            "Percentile: 85, Seuil: 0.0003, F1-Score: 0.3846\n",
            "Percentile: 86, Seuil: 0.0006, F1-Score: 0.4082\n",
            "Percentile: 87, Seuil: 0.0022, F1-Score: 0.4348\n",
            "Percentile: 88, Seuil: 0.0049, F1-Score: 0.4651\n",
            "Percentile: 89, Seuil: 0.0050, F1-Score: 0.4878\n",
            "Percentile: 90, Seuil: 0.0051, F1-Score: 0.5263\n",
            "Percentile: 91, Seuil: 0.0073, F1-Score: 0.5714\n",
            "Percentile: 92, Seuil: 0.0175, F1-Score: 0.6250\n",
            "Percentile: 93, Seuil: 0.0482, F1-Score: 0.6667\n",
            "Percentile: 94, Seuil: 0.1090, F1-Score: 0.7407\n",
            "Percentile: 95, Seuil: 0.1812, F1-Score: 0.8333\n",
            "Percentile: 96, Seuil: 0.9971, F1-Score: 0.8571\n",
            "Percentile: 97, Seuil: 0.9988, F1-Score: 0.7368\n",
            "Percentile: 98, Seuil: 0.9995, F1-Score: 0.5000\n",
            "Percentile: 99, Seuil: 0.9998, F1-Score: 0.1538\n",
            "\n",
            "Seuil optimal: 0.9970806221338183\n",
            "\n",
            "=== Matrice de confusion optimale ===\n",
            "TP: 9, FP: 2\n",
            "FN: 264, TN: 1\n",
            "\n",
            "=== Métriques de performance optimales ===\n",
            "|           |   Hybrid FLAGS PCA |\n",
            "|:----------|-------------------:|\n",
            "| FPR       |             0.0075 |\n",
            "| TPR       |             0.9    |\n",
            "| TNR       |             0.9925 |\n",
            "| FNR       |             0.1    |\n",
            "| Precision |             0.8182 |\n",
            "| Accuracy  |             0.9891 |\n",
            "| ErrorRate |             0.0109 |\n",
            "| F1-Score  |             0.8571 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iMSfvnXuwKHc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}